{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataParallel(nn.DataParallel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super(MyDataParallel, self).__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        try:\n",
    "            return super(MyDataParallel, self).__setattr__(name, value)\n",
    "        except AttributeError:\n",
    "            return setattr(self.module, name, value)\n",
    "        \n",
    "class ReLULinearDeepLiftModel(nn.Module):\n",
    "    r\"\"\"\n",
    "        Simple architecture similar to:\n",
    "        https://github.com/marcoancona/DeepExplain/blob/master/deepexplain/tests/test_tensorflow.py#L65\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(3, 1, bias=False)\n",
    "        self.l2 = nn.Linear(3, 1, bias=False)\n",
    "        self.l1.weight = nn.Parameter(torch.tensor([[3.0, 1.0, 0.0]]))\n",
    "        self.l2.weight = nn.Parameter(torch.tensor([[2.0, 3.0, 0.0]]))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l3 = nn.Linear(2, 1, bias=False)\n",
    "        self.l3.weight = nn.Parameter(torch.tensor([[1.0, 1.0]]))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.l3(self.relu(torch.cat([self.l1(x1), self.l2(x2)], axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_model = MyDataParallel(ReLULinearDeepLiftModel()).cuda()\n",
    "\n",
    "model = ReLULinearDeepLiftModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_hook(module, baseline_inputs):\n",
    "    inputs = baseline_inputs[0]\n",
    "    baselines = baseline_inputs[1]\n",
    "    res = tuple(torch.cat([input, baseline]) for input, baseline in zip(inputs, baselines))\n",
    "\n",
    "    print('res: ', res)\n",
    "\n",
    "    return res\n",
    "  \n",
    "dp_model.module.register_forward_pre_hook(pre_hook)\n",
    "\n",
    "model.register_forward_pre_hook(pre_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def _forward_hook(module, inputs, outputs):\n",
    "    print('inputs: ', inputs)\n",
    "\n",
    "dp_model.module.relu.register_forward_hook(_forward_hook)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp1 = torch.tensor([[-10.0, 1.0, -5.0], [-23.0, 22.0, -12.0], [-10.0, 1.0, -5.0]], requires_grad=True).cuda()\n",
    "inp2 = torch.tensor([[3.0, 3.0, 1.0], [13.0, 23.0, 31.0], [3.0, 3.0, 1.0]], requires_grad=True).cuda()\n",
    "\n",
    "base1 = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], requires_grad=True).cuda()\n",
    "base2 = torch.tensor([[1.0, 1.0, 0.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], requires_grad=True).cuda()\n",
    "\n",
    "inputs = (inp1, inp2)\n",
    "baselines = (base1, base2)\n",
    "pred = dp_model(*inputs)\n",
    "\n",
    "print('pred: ', pred)\n",
    "\n",
    "# ----- \n",
    "\n",
    "inp1 = torch.tensor([[-10.0, 1.0, -5.0], [-23.0, 22.0, -12.0], [-10.0, 1.0, -5.0]], requires_grad=True)\n",
    "inp2 = torch.tensor([[3.0, 3.0, 1.0], [13.0, 23.0, 31.0], [3.0, 3.0, 1.0]], requires_grad=True)\n",
    "\n",
    "base1 = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], requires_grad=True)\n",
    "base2 = torch.tensor([[1.0, 1.0, 0.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], requires_grad=True)\n",
    "\n",
    "inputs = (inp1, inp2)\n",
    "\n",
    "baselines = (base1, base2)\n",
    "pred = model(*inputs)\n",
    "print('pred: ', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(1,2,3)\n",
    "t2 = torch.rand(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cat([t1, t2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(50, 80) #tensor of size 50 x 80\n",
    "b = torch.split(a, 40, dim=1) # it returns a tuple\n",
    "\n",
    "spl = t.split(2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_hook(module, inputs, outputs):\n",
    "    if type(module) == nn.ReLU:\n",
    "        print(inputs)\n",
    "        print(outputs)\n",
    "    module.a = nn.Parameter(torch.zeros(1,2))\n",
    "    \n",
    "def _forward_hook_on_relu(module):\n",
    "        module.register_forward_hook(_forward_hook)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_forward_hook_on_relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d19dfaa80725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_forward_hook_on_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_forward_hook_on_relu' is not defined"
     ]
    }
   ],
   "source": [
    "model = ReLULinearDeepLiftModel()\n",
    "\n",
    "inp1 = torch.tensor([[-10.,   1.,  -5.], [ -0.,   0.,  -0.]], requires_grad=True, device=torch.device('cuda:0'))\n",
    "inp2 = torch.tensor([[3., 3., 1.], [0., 0., 0.]], requires_grad=True)\n",
    "\n",
    "#tensor([[-10.,   1.,  -5.],\n",
    "#        [ -0.,   0.,  -0.]], device='cuda:0', grad_fn=<CatBackward>), tensor([[3., 3., 1.],\n",
    "#        [0., 0., 0.]], device='cuda:0', grad_fn=<CatBackward>)\n",
    "inputs = (inp1, inp2)\n",
    "\n",
    "\n",
    "model.apply(_forward_hook_on_relu)\n",
    "model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_func:  ReLULinearDeepLiftModel(\n",
      "  (l1): Linear(in_features=3, out_features=1, bias=False)\n",
      "  (l2): Linear(in_features=3, out_features=1, bias=False)\n",
      "  (relu): ReLU()\n",
      "  (l3): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/narine/captum/captum/attr/_core/deep_lift.py:239: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  after the attribution is finished\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, -0.0000, 0.0000],\n",
       "         [5.7000, 2.0000, 0.0000],\n",
       "         [3.3000, 1.2000, 0.0000],\n",
       "         [5.7000, 2.0000, 0.0000]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[6.0000, 9.0000, 0.0000],\n",
       "         [2.4000, 9.0000, 0.0000],\n",
       "         [2.2000, 3.6000, 0.0000],\n",
       "         [2.4000, 9.0000, 0.0000]], device='cuda:0', grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import DeepLift\n",
    "net = ReLULinearDeepLiftModel().cuda()\n",
    "inp1 = torch.tensor([[-10.0, 1.0, -5.0], [1.9, 2.0, 1.9],[1.1, 1.2, 1.3], [1.9, 2.0, 1.9]], requires_grad=True).cuda()\n",
    "inp2 = torch.tensor([[3.0, 3.0, 1.0], [1.2, 3.0, 2.3],[1.1, 1.2, 1.3], [1.2, 3.0, 2.3]], requires_grad=True).cuda()\n",
    "dl = DeepLift(net)\n",
    "dl.attribute((inp1, inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_func:  DataParallel(\n",
      "  (module): ReLULinearDeepLiftModel(\n",
      "    (l1): Linear(in_features=3, out_features=1, bias=False)\n",
      "    (l2): Linear(in_features=3, out_features=1, bias=False)\n",
      "    (relu): ReLU()\n",
      "    (l3): Linear(in_features=2, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, -0.0000, 0.0000],\n",
       "         [5.7000, 2.0000, 0.0000],\n",
       "         [3.3000, 1.2000, 0.0000],\n",
       "         [5.7000, 2.0000, 0.0000]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[6.0000, 9.0000, 0.0000],\n",
       "         [2.4000, 9.0000, 0.0000],\n",
       "         [2.2000, 3.6000, 0.0000],\n",
       "         [2.4000, 9.0000, 0.0000]], device='cuda:0', grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import DeepLift, LayerDeepLift\n",
    "net = nn.DataParallel(ReLULinearDeepLiftModel().cuda())\n",
    "inp1 = torch.tensor([[-10.0, 1.0, -5.0], [1.9, 2.0, 1.9], [1.1, 1.2, 1.3], [1.9, 2.0, 1.9]], requires_grad=True).cuda()\n",
    "inp2 = torch.tensor([[3.0, 3.0, 1.0], [1.2, 3.0, 2.3], [1.1, 1.2, 1.3], [1.2, 3.0, 2.3]], requires_grad=True).cuda()\n",
    "dl = DeepLift(net)\n",
    "dl.attribute((inp1, inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_func:  DataParallel(\n",
      "  (module): ReLULinearDeepLiftModel(\n",
      "    (l1): Linear(in_features=3, out_features=1, bias=False)\n",
      "    (l2): Linear(in_features=3, out_features=1, bias=False)\n",
      "    (relu): ReLU()\n",
      "    (l3): Linear(in_features=2, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15.0000],\n",
       "        [11.4000],\n",
       "        [ 5.8000],\n",
       "        [11.4000]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import DeepLift, LayerDeepLift\n",
    "net = nn.DataParallel(ReLULinearDeepLiftModel().cuda())\n",
    "inp1 = torch.tensor([[-10.0, 1.0, -5.0], [1.9, 2.0, 1.9], [1.1, 1.2, 1.3], [1.9, 2.0, 1.9]], requires_grad=True).cuda()\n",
    "inp2 = torch.tensor([[3.0, 3.0, 1.0], [1.2, 3.0, 2.3], [1.1, 1.2, 1.3], [1.2, 3.0, 2.3]], requires_grad=True).cuda()\n",
    "dl = LayerDeepLift(net, net.module.l2)\n",
    "dl.attribute((inp1, inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_func:  ReLULinearDeepLiftModel(\n",
      "  (l1): Linear(in_features=3, out_features=1, bias=False)\n",
      "  (l2): Linear(in_features=3, out_features=1, bias=False)\n",
      "  (relu): ReLU()\n",
      "  (l3): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15.0000],\n",
       "        [11.4000],\n",
       "        [ 5.8000],\n",
       "        [11.4000]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from captum.attr import DeepLift, LayerDeepLift\n",
    "net = ReLULinearDeepLiftModel().cuda()\n",
    "inp1 = torch.tensor([[-10.0, 1.0, -5.0], [1.9, 2.0, 1.9], [1.1, 1.2, 1.3], [1.9, 2.0, 1.9]], requires_grad=True).cuda()\n",
    "inp2 = torch.tensor([[3.0, 3.0, 1.0], [1.2, 3.0, 2.3], [1.1, 1.2, 1.3], [1.2, 3.0, 2.3]], requires_grad=True).cuda()\n",
    "dl = LayerDeepLift(net, net.l2)\n",
    "dl.attribute((inp1, inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2633861d99e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "k.view(k.shape[1:], 2)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1],[2],[3],[4],[5],[6]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.view((3,)+ (2,) + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.0000,   1.0000,  -5.0000],\n",
       "        [  1.9000,   2.0000,   1.9000],\n",
       "        [  1.1000,   1.2000,   1.3000],\n",
       "        [  1.9000,   2.0000,   1.9000]], device='cuda:0',\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = lambda out: out.chunk(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.0000,   1.0000,  -5.0000],\n",
       "        [  1.9000,   2.0000,   1.9000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1[0: int(len(inp1) / 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.0000,   1.0000,  -5.0000],\n",
       "        [  1.9000,   2.0000,   1.9000],\n",
       "        [  1.1000,   1.2000,   1.3000],\n",
       "        [  1.9000,   2.0000,   1.9000],\n",
       "        [  3.0000,   3.0000,   1.0000],\n",
       "        [  1.2000,   3.0000,   2.3000],\n",
       "        [  1.1000,   1.2000,   1.3000],\n",
       "        [  1.2000,   3.0000,   2.3000]], device='cuda:0',\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((inp1,inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a6dfc7fcfe48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "inp = torch.tensor(0)\n",
    "inp.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
