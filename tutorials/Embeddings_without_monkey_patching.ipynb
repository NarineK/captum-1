{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel.scatter_gather import scatter_kwargs, scatter\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel, LayerIntegratedGradients, GradientShap, LayerConductance, GradientAttribution\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from captum.attr._utils.approximation_methods import approximation_parameters\n",
    "from captum.attr._utils.batching import _batched_operator\n",
    "from captum.attr._utils.common import (\n",
    "    _validate_input,\n",
    "    _format_additional_forward_args,\n",
    "    _format_attributions,\n",
    "    _format_input_baseline,\n",
    "    _reshape_and_sum,\n",
    "    _expand_additional_forward_args,\n",
    "    _expand_target,\n",
    ")\n",
    "#from  captum.attr._utils.batching import _sort_key_list\n",
    "\n",
    "from captum.attr._utils.gradient import compute_gradients, _forward_layer_distributed_eval, _gather_distributed_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2308],\n",
       "        [0.3428],\n",
       "        [0.3436]], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasicEmbeddingModel(nn.Module):\n",
    "    r\"\"\"\n",
    "    Implements basic model with nn.Embedding layer. This simple model\n",
    "    will help us to test nested InterpretableEmbedding layers\n",
    "    The model has the following structure:\n",
    "    BasicEmbeddingModel(\n",
    "      (embedding1): Embedding(30, 100)\n",
    "      (embedding2): TextModule(\n",
    "        (inner_embedding): Embedding(30, 100)\n",
    "      )\n",
    "      (linear1): Linear(in_features=100, out_features=256, bias=True)\n",
    "      (relu): ReLU()\n",
    "      (linear2): Linear(in_features=256, out_features=1, bias=True)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_embeddings=30, embedding_dim=200, hidden_dim=256, output_dim=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedding1 = self.embedding1(input)\n",
    "        return self.linear2(self.relu(self.linear1(embedding1))).squeeze(1)\n",
    "        \n",
    "\n",
    "#device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load model\n",
    "model = BasicEmbeddingModel().cuda()\n",
    "#model = nn.DataParallel(model)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "input = torch.tensor([[2],[1], [4]]).cuda()\n",
    "ref = torch.tensor([[0], [3], [5]]).cuda()\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape:  torch.Size([150, 1])\n",
      "output.shape:  torch.Size([150, 1])\n",
      "output.shape:  torch.Size([150, 1])\n",
      "output.shape:  torch.Size([3, 1])\n",
      "output.shape:  torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.8434e-02,  2.4336e-03, -7.7239e-03,  2.9334e-02, -2.2649e-02,\n",
       "            1.1067e-02,  9.4211e-02, -2.4433e-03,  6.1734e-03,  3.3512e-04,\n",
       "            1.4698e-02,  4.3736e-03,  1.1171e-03, -1.0069e-02,  3.6302e-02,\n",
       "           -5.7423e-02,  2.2184e-03,  1.6446e-03,  8.4808e-03,  3.1292e-03,\n",
       "           -1.2136e-02,  4.6877e-04, -2.5499e-02,  6.4064e-03,  9.3984e-04,\n",
       "           -6.3043e-03,  4.6368e-04, -1.8031e-03,  2.6459e-02, -8.8340e-03,\n",
       "           -5.2754e-02,  5.9586e-03, -3.9220e-03,  1.7803e-02,  1.0983e-03,\n",
       "            4.2014e-03,  1.0652e-02, -9.1668e-04, -1.0238e-02,  2.2246e-03,\n",
       "           -4.6415e-03, -3.6420e-03, -1.9941e-03, -8.7421e-03,  2.6622e-03,\n",
       "            3.3955e-03,  3.0525e-03, -2.4556e-03,  6.9845e-03, -1.7373e-02,\n",
       "           -1.7561e-03,  1.4941e-02,  9.3275e-04,  1.1926e-02, -1.9599e-02,\n",
       "           -2.4048e-03, -1.6024e-03, -5.4949e-02,  1.3060e-03, -3.2248e-03,\n",
       "           -5.4806e-03,  5.2448e-03, -1.4473e-02, -2.2163e-02, -7.5677e-04,\n",
       "            1.7151e-02,  2.8610e-02, -1.2109e-02, -6.1680e-03, -1.4464e-02,\n",
       "           -3.8158e-03, -5.4367e-03,  2.9921e-02,  1.5766e-02, -2.6010e-02,\n",
       "            2.6358e-02, -1.3129e-02, -2.2076e-02,  8.8843e-02,  2.0744e-02,\n",
       "            9.5298e-03, -2.7945e-03, -6.9916e-03,  2.0244e-02,  2.4825e-03,\n",
       "           -1.6740e-03, -7.3024e-02, -1.7577e-04, -7.1744e-03, -5.7483e-03,\n",
       "            3.4522e-03, -6.9274e-04, -5.0210e-04,  3.1145e-03,  5.8898e-03,\n",
       "            2.7594e-02,  1.8447e-02,  4.9197e-03,  1.4004e-02,  2.3237e-02,\n",
       "            2.8809e-02, -2.5976e-02, -1.5977e-02, -3.8284e-02,  3.6380e-04,\n",
       "            8.8691e-03,  1.6445e-02,  4.0788e-03,  3.4987e-02,  2.6626e-03,\n",
       "            1.7190e-02,  6.6745e-04, -1.0184e-02,  3.7737e-04,  1.5365e-03,\n",
       "            5.2022e-02,  7.1337e-03,  1.4123e-02,  1.6062e-02,  3.5318e-02,\n",
       "           -4.1483e-02, -6.0582e-02,  2.5689e-02, -1.8362e-02,  2.4362e-03,\n",
       "            4.5144e-02, -1.7218e-02,  1.7694e-02, -5.5089e-02,  5.2507e-02,\n",
       "           -7.3474e-03,  2.4512e-02, -5.8766e-04,  1.3123e-03, -2.7893e-02,\n",
       "            1.5434e-02,  1.8779e-02, -8.4719e-03,  7.0336e-03, -1.9312e-02,\n",
       "           -8.6415e-03,  3.2546e-03, -2.9651e-03,  7.6358e-04,  3.4480e-04,\n",
       "           -1.9009e-03,  1.9176e-02,  9.2321e-04, -3.9790e-02, -1.4679e-03,\n",
       "            3.7811e-02, -4.8607e-03,  2.1917e-02,  3.7688e-04, -9.9846e-03,\n",
       "            3.7381e-04,  2.3911e-04,  2.0522e-02,  2.0710e-02,  2.9220e-03,\n",
       "           -5.6018e-03,  1.3826e-02, -7.7659e-04, -3.3207e-03,  5.4066e-03,\n",
       "           -4.2667e-03,  6.3334e-02,  6.1066e-03,  3.3669e-03, -2.6791e-03,\n",
       "           -3.9893e-02,  1.5091e-02, -2.1363e-03,  2.0709e-03,  1.0810e-02,\n",
       "            1.9237e-02, -2.9222e-02,  1.6742e-02,  2.5117e-03, -1.0512e-04,\n",
       "           -2.8348e-02,  8.6809e-04,  4.1906e-03,  7.4599e-04,  1.2383e-02,\n",
       "           -2.1938e-02,  2.2796e-03,  1.5324e-02, -1.4372e-02,  3.8163e-02,\n",
       "            1.6024e-02,  1.6837e-02, -2.4629e-02, -5.9508e-03,  2.5682e-02,\n",
       "            9.5364e-04, -2.6437e-03, -2.3604e-02,  8.9960e-04, -3.5762e-04]],\n",
       " \n",
       "         [[-9.4080e-03,  5.2668e-03,  2.4082e-02,  9.2574e-03,  2.0814e-03,\n",
       "           -2.8592e-02, -2.3681e-02,  9.0220e-04, -1.4512e-02, -7.9945e-03,\n",
       "           -3.4608e-03, -1.5316e-02,  1.5630e-02, -3.5687e-03,  1.5716e-02,\n",
       "           -1.1081e-02, -1.9140e-03, -3.3323e-03,  4.3679e-04,  1.1025e-03,\n",
       "           -5.0147e-04, -9.3276e-03,  1.8597e-02, -1.5270e-02,  4.1122e-04,\n",
       "           -5.2671e-02,  3.8684e-03, -4.5219e-02,  1.0312e-02,  3.8622e-03,\n",
       "            5.8741e-03,  1.3426e-02, -5.0408e-04,  8.5145e-02,  4.6586e-02,\n",
       "            4.1392e-02,  2.1713e-02,  2.8872e-02,  1.2753e-02,  1.7017e-03,\n",
       "            1.9396e-02, -3.1772e-03, -2.7809e-02, -5.4604e-05, -1.4231e-02,\n",
       "            1.8353e-05, -8.1880e-03,  1.1969e-02,  9.4941e-04, -1.5065e-02,\n",
       "           -1.0758e-03, -5.2995e-02, -2.4897e-04, -7.5294e-03,  3.5797e-03,\n",
       "           -1.0527e-02,  1.0784e-02,  5.0761e-02, -2.0098e-02,  7.7584e-03,\n",
       "            7.0632e-03,  1.0227e-02,  1.6964e-01,  2.1299e-02,  6.1585e-03,\n",
       "            1.2625e-03,  4.7494e-03, -3.3035e-02,  9.6537e-03, -1.2701e-02,\n",
       "            2.3434e-03, -8.8886e-04,  6.0195e-04, -1.2242e-02,  8.5861e-03,\n",
       "           -1.6521e-03,  4.1385e-03,  1.3668e-03, -7.3047e-03, -2.2533e-02,\n",
       "            3.0917e-02,  5.3473e-04,  1.9465e-02,  9.7168e-03,  5.0074e-02,\n",
       "           -3.9969e-03,  7.1013e-03, -1.3921e-03, -1.1730e-02, -4.7746e-02,\n",
       "            1.6167e-02, -3.5332e-03, -2.9933e-03, -8.3722e-03, -1.9833e-03,\n",
       "           -8.5535e-03,  2.2016e-02,  7.5896e-03,  2.3160e-03,  9.7009e-03,\n",
       "           -6.1654e-03,  5.0980e-03, -1.9339e-02, -1.3930e-02,  2.8213e-02,\n",
       "            8.2376e-04, -9.2916e-03,  1.1042e-03, -2.3621e-02,  7.6599e-03,\n",
       "            8.3191e-04,  1.1326e-03,  2.5942e-02,  2.2451e-03,  1.4125e-03,\n",
       "            3.2118e-02,  3.9200e-03,  3.0604e-03, -9.7345e-04,  5.2587e-03,\n",
       "            4.6260e-02,  4.4582e-02,  1.4308e-02,  2.9567e-02,  1.0341e-02,\n",
       "           -2.5082e-03, -2.4567e-02, -5.3316e-04,  4.0768e-02,  1.3410e-02,\n",
       "           -2.8722e-02, -1.4921e-02,  7.6955e-03, -7.2807e-03, -4.8398e-03,\n",
       "           -1.5638e-02, -3.2571e-03,  1.0078e-02,  8.8877e-03, -1.0616e-02,\n",
       "            1.9201e-04,  4.7869e-03,  7.6771e-03, -2.0228e-02,  1.9961e-04,\n",
       "            2.9737e-02,  1.5060e-03,  3.0772e-03,  4.9462e-02, -2.0164e-02,\n",
       "            7.2670e-03,  4.7180e-03,  3.2135e-02,  2.3778e-03, -1.5088e-02,\n",
       "           -4.6102e-04, -1.0990e-02, -2.0417e-02,  1.1567e-03,  2.3239e-02,\n",
       "            1.8093e-04, -1.3742e-03,  9.9396e-03,  1.5547e-02,  1.1223e-04,\n",
       "            1.4432e-02, -1.6222e-03, -1.2030e-02,  4.1439e-03,  2.2759e-02,\n",
       "           -8.8496e-04, -5.2928e-02,  7.8545e-03, -1.0793e-03,  3.6379e-04,\n",
       "            1.2993e-02, -6.6117e-03, -3.0101e-03,  3.3173e-04, -2.5878e-02,\n",
       "           -7.0100e-03, -6.0765e-03, -5.6908e-02, -6.9117e-02, -5.9733e-03,\n",
       "           -7.1765e-03,  2.8758e-02, -7.0431e-04, -4.7443e-03, -2.2998e-02,\n",
       "           -7.7887e-02, -1.7900e-03,  2.2912e-03,  5.4989e-03, -1.0063e-02,\n",
       "           -2.1349e-02,  2.5279e-03, -3.9801e-03, -1.4251e-02,  1.5382e-03]],\n",
       " \n",
       "         [[-2.3286e-03,  2.0283e-03, -6.6744e-04, -1.6559e-02,  6.5720e-03,\n",
       "           -1.2079e-02,  4.9629e-02,  2.5286e-04,  2.2596e-04,  2.7547e-02,\n",
       "            5.7574e-03,  5.1312e-02, -6.2244e-03, -4.7173e-03,  2.3380e-02,\n",
       "           -9.4676e-02,  1.9332e-02, -8.1813e-03,  1.0420e-02, -1.2491e-02,\n",
       "           -5.4599e-03,  3.1452e-03,  1.4201e-03, -1.2396e-02,  1.2030e-03,\n",
       "            1.2347e-02,  4.4215e-03,  4.1918e-03,  2.9093e-03,  1.3054e-04,\n",
       "           -2.4323e-02,  8.2645e-03, -5.1275e-03,  1.6533e-02,  2.5841e-02,\n",
       "           -2.3577e-02,  1.5053e-02, -3.2556e-02,  1.4776e-02,  2.1859e-02,\n",
       "            1.4961e-03,  2.6019e-03,  2.0043e-03, -7.3157e-03,  2.5654e-02,\n",
       "            2.0396e-02, -2.8950e-02, -1.4456e-02,  8.1608e-03,  7.9425e-04,\n",
       "           -6.1699e-03,  1.5219e-02, -4.3468e-05, -5.1164e-02,  3.3460e-04,\n",
       "            7.7237e-03, -8.1609e-03,  3.0864e-04,  1.5822e-02,  5.9293e-03,\n",
       "           -2.6189e-03,  5.3244e-03,  1.0650e-02,  8.5100e-03,  1.9794e-02,\n",
       "            4.8995e-03, -1.1987e-02,  2.1339e-02,  4.5367e-05, -1.5962e-02,\n",
       "            3.0456e-03, -4.0318e-03,  2.5835e-03, -3.8167e-03,  3.0582e-02,\n",
       "            1.1173e-03,  1.8132e-02,  1.5550e-02,  2.2462e-02,  2.5909e-03,\n",
       "           -2.1253e-02,  1.6961e-02,  8.3556e-03, -1.1336e-02, -3.3049e-02,\n",
       "            3.3460e-02,  2.2388e-03,  4.5869e-02, -1.3988e-02, -5.7786e-03,\n",
       "            1.1054e-03, -8.2625e-03, -5.8029e-04,  6.2101e-03, -3.8918e-03,\n",
       "           -3.8307e-02, -4.3026e-03,  6.4529e-04, -1.1102e-02, -1.6856e-03,\n",
       "            1.3104e-02,  1.0549e-02,  5.8109e-03,  2.9451e-03, -1.8629e-02,\n",
       "            1.9396e-03, -1.6939e-03,  8.8995e-03, -1.5541e-02,  3.9094e-02,\n",
       "           -3.7681e-03,  2.6657e-02,  1.0847e-04,  1.5947e-02, -3.4866e-02,\n",
       "            7.6270e-02, -9.3866e-03,  3.8811e-04, -2.4564e-03,  4.2912e-02,\n",
       "           -1.6110e-02,  4.3428e-02, -1.9180e-03,  2.2523e-04, -1.1059e-04,\n",
       "           -1.0873e-02, -7.2969e-03,  9.6337e-04,  3.9888e-03, -2.0258e-05,\n",
       "            7.2102e-03,  1.5817e-02, -5.9354e-02, -8.2039e-04, -7.7920e-03,\n",
       "            5.5584e-03,  2.5050e-04,  1.6252e-03, -2.4397e-02, -2.8518e-03,\n",
       "           -2.2400e-02, -4.3824e-03,  3.9032e-03,  8.9161e-03, -3.5756e-04,\n",
       "           -2.1744e-03, -9.7966e-04,  1.7553e-02,  7.4509e-03,  4.4980e-02,\n",
       "           -3.6480e-03,  2.4134e-03,  4.5234e-03,  8.6861e-03,  2.0400e-02,\n",
       "           -4.3157e-05, -1.6100e-03, -2.1848e-03, -2.2111e-03,  1.4692e-02,\n",
       "            2.4300e-02,  3.3802e-03,  2.3725e-02,  4.8159e-03, -1.1157e-02,\n",
       "           -3.8345e-03,  6.5152e-03,  5.9454e-03, -4.0439e-02,  4.6724e-03,\n",
       "           -6.1756e-03,  1.6890e-02,  1.1168e-02, -9.9416e-04,  2.0177e-02,\n",
       "           -1.3920e-03,  9.9222e-03,  6.8456e-03,  3.6914e-03,  1.6102e-03,\n",
       "            3.3946e-03,  1.1744e-03, -7.5108e-03,  5.4903e-02, -1.2558e-02,\n",
       "            1.6928e-02,  2.2004e-03, -1.8285e-03, -1.1653e-02, -1.1406e-02,\n",
       "           -2.6765e-02,  2.5559e-03, -6.5925e-03, -2.6651e-03, -1.6569e-02,\n",
       "           -5.8750e-02,  2.6631e-02,  1.4833e-02,  1.7115e-03,  2.6244e-03]]],\n",
       "        device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([-0.0020, -0.0051,  0.0061], device='cuda:0'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(model, model.embedding1)\n",
    "attrs = lig.attribute(input, baselines=ref, return_convergence_delta=True)\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/narine/captum/captum/attr/_models/base.py:181: UserWarning: In order to make embedding layers more interpretable they will\n",
      "        be replaced with an interpretable embedding layer which wraps the\n",
      "        original embedding layer and takes word embedding vectors as inputs of\n",
      "        the forward function. This allows to generate baselines for word\n",
      "        embeddings and compute attributions for each embedding dimension.\n",
      "        The original embedding layer must be set\n",
      "        back by calling `remove_interpretable_embedding_layer` function\n",
      "        after model interpretation is finished.\n",
      "  after model interpretation is finished.\"\"\"\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "interpret_layer = configure_interpretable_embedding_layer(model, 'embedding1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = interpret_layer.indices_to_embeddings(input)\n",
    "ref_emb = interpret_layer.indices_to_embeddings(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape:  torch.Size([150, 1])\n",
      "output.shape:  torch.Size([3, 1])\n",
      "output.shape:  torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.8434e-02,  2.4336e-03, -7.7239e-03,  2.9334e-02, -2.2649e-02,\n",
       "            1.1067e-02,  9.4211e-02, -2.4433e-03,  6.1734e-03,  3.3512e-04,\n",
       "            1.4698e-02,  4.3736e-03,  1.1171e-03, -1.0069e-02,  3.6302e-02,\n",
       "           -5.7423e-02,  2.2184e-03,  1.6446e-03,  8.4808e-03,  3.1292e-03,\n",
       "           -1.2136e-02,  4.6877e-04, -2.5499e-02,  6.4064e-03,  9.3984e-04,\n",
       "           -6.3043e-03,  4.6368e-04, -1.8031e-03,  2.6459e-02, -8.8340e-03,\n",
       "           -5.2754e-02,  5.9586e-03, -3.9220e-03,  1.7803e-02,  1.0983e-03,\n",
       "            4.2014e-03,  1.0652e-02, -9.1668e-04, -1.0238e-02,  2.2246e-03,\n",
       "           -4.6415e-03, -3.6420e-03, -1.9941e-03, -8.7421e-03,  2.6622e-03,\n",
       "            3.3955e-03,  3.0525e-03, -2.4556e-03,  6.9845e-03, -1.7373e-02,\n",
       "           -1.7561e-03,  1.4941e-02,  9.3275e-04,  1.1926e-02, -1.9599e-02,\n",
       "           -2.4048e-03, -1.6024e-03, -5.4949e-02,  1.3060e-03, -3.2248e-03,\n",
       "           -5.4806e-03,  5.2448e-03, -1.4473e-02, -2.2163e-02, -7.5677e-04,\n",
       "            1.7151e-02,  2.8610e-02, -1.2109e-02, -6.1680e-03, -1.4464e-02,\n",
       "           -3.8158e-03, -5.4367e-03,  2.9921e-02,  1.5766e-02, -2.6010e-02,\n",
       "            2.6358e-02, -1.3129e-02, -2.2076e-02,  8.8843e-02,  2.0744e-02,\n",
       "            9.5298e-03, -2.7945e-03, -6.9916e-03,  2.0244e-02,  2.4825e-03,\n",
       "           -1.6740e-03, -7.3024e-02, -1.7577e-04, -7.1744e-03, -5.7483e-03,\n",
       "            3.4522e-03, -6.9274e-04, -5.0210e-04,  3.1145e-03,  5.8898e-03,\n",
       "            2.7594e-02,  1.8447e-02,  4.9197e-03,  1.4004e-02,  2.3237e-02,\n",
       "            2.8809e-02, -2.5976e-02, -1.5977e-02, -3.8284e-02,  3.6380e-04,\n",
       "            8.8691e-03,  1.6445e-02,  4.0788e-03,  3.4987e-02,  2.6626e-03,\n",
       "            1.7190e-02,  6.6745e-04, -1.0184e-02,  3.7737e-04,  1.5365e-03,\n",
       "            5.2022e-02,  7.1337e-03,  1.4123e-02,  1.6062e-02,  3.5318e-02,\n",
       "           -4.1483e-02, -6.0582e-02,  2.5689e-02, -1.8362e-02,  2.4362e-03,\n",
       "            4.5144e-02, -1.7218e-02,  1.7694e-02, -5.5089e-02,  5.2507e-02,\n",
       "           -7.3474e-03,  2.4512e-02, -5.8766e-04,  1.3123e-03, -2.7893e-02,\n",
       "            1.5434e-02,  1.8779e-02, -8.4719e-03,  7.0336e-03, -1.9312e-02,\n",
       "           -8.6415e-03,  3.2546e-03, -2.9651e-03,  7.6358e-04,  3.4480e-04,\n",
       "           -1.9009e-03,  1.9176e-02,  9.2321e-04, -3.9790e-02, -1.4679e-03,\n",
       "            3.7811e-02, -4.8607e-03,  2.1917e-02,  3.7688e-04, -9.9846e-03,\n",
       "            3.7381e-04,  2.3911e-04,  2.0522e-02,  2.0710e-02,  2.9220e-03,\n",
       "           -5.6018e-03,  1.3826e-02, -7.7659e-04, -3.3207e-03,  5.4066e-03,\n",
       "           -4.2667e-03,  6.3334e-02,  6.1066e-03,  3.3669e-03, -2.6791e-03,\n",
       "           -3.9893e-02,  1.5091e-02, -2.1363e-03,  2.0709e-03,  1.0810e-02,\n",
       "            1.9237e-02, -2.9222e-02,  1.6742e-02,  2.5117e-03, -1.0512e-04,\n",
       "           -2.8348e-02,  8.6809e-04,  4.1906e-03,  7.4599e-04,  1.2383e-02,\n",
       "           -2.1938e-02,  2.2796e-03,  1.5324e-02, -1.4372e-02,  3.8163e-02,\n",
       "            1.6024e-02,  1.6837e-02, -2.4629e-02, -5.9508e-03,  2.5682e-02,\n",
       "            9.5364e-04, -2.6437e-03, -2.3604e-02,  8.9960e-04, -3.5762e-04]],\n",
       " \n",
       "         [[-9.4080e-03,  5.2668e-03,  2.4082e-02,  9.2574e-03,  2.0814e-03,\n",
       "           -2.8592e-02, -2.3681e-02,  9.0220e-04, -1.4512e-02, -7.9945e-03,\n",
       "           -3.4608e-03, -1.5316e-02,  1.5630e-02, -3.5687e-03,  1.5716e-02,\n",
       "           -1.1081e-02, -1.9140e-03, -3.3323e-03,  4.3679e-04,  1.1025e-03,\n",
       "           -5.0147e-04, -9.3276e-03,  1.8597e-02, -1.5270e-02,  4.1122e-04,\n",
       "           -5.2671e-02,  3.8684e-03, -4.5219e-02,  1.0312e-02,  3.8622e-03,\n",
       "            5.8741e-03,  1.3426e-02, -5.0408e-04,  8.5145e-02,  4.6586e-02,\n",
       "            4.1392e-02,  2.1713e-02,  2.8872e-02,  1.2753e-02,  1.7017e-03,\n",
       "            1.9396e-02, -3.1772e-03, -2.7809e-02, -5.4604e-05, -1.4231e-02,\n",
       "            1.8353e-05, -8.1880e-03,  1.1969e-02,  9.4941e-04, -1.5065e-02,\n",
       "           -1.0758e-03, -5.2995e-02, -2.4897e-04, -7.5294e-03,  3.5797e-03,\n",
       "           -1.0527e-02,  1.0784e-02,  5.0761e-02, -2.0098e-02,  7.7584e-03,\n",
       "            7.0632e-03,  1.0227e-02,  1.6964e-01,  2.1299e-02,  6.1585e-03,\n",
       "            1.2625e-03,  4.7494e-03, -3.3035e-02,  9.6537e-03, -1.2701e-02,\n",
       "            2.3434e-03, -8.8886e-04,  6.0195e-04, -1.2242e-02,  8.5861e-03,\n",
       "           -1.6521e-03,  4.1385e-03,  1.3668e-03, -7.3047e-03, -2.2533e-02,\n",
       "            3.0917e-02,  5.3473e-04,  1.9465e-02,  9.7168e-03,  5.0074e-02,\n",
       "           -3.9969e-03,  7.1013e-03, -1.3921e-03, -1.1730e-02, -4.7746e-02,\n",
       "            1.6167e-02, -3.5332e-03, -2.9933e-03, -8.3722e-03, -1.9833e-03,\n",
       "           -8.5535e-03,  2.2016e-02,  7.5896e-03,  2.3160e-03,  9.7009e-03,\n",
       "           -6.1654e-03,  5.0980e-03, -1.9339e-02, -1.3930e-02,  2.8213e-02,\n",
       "            8.2376e-04, -9.2916e-03,  1.1042e-03, -2.3621e-02,  7.6599e-03,\n",
       "            8.3191e-04,  1.1326e-03,  2.5942e-02,  2.2451e-03,  1.4125e-03,\n",
       "            3.2118e-02,  3.9200e-03,  3.0604e-03, -9.7345e-04,  5.2587e-03,\n",
       "            4.6260e-02,  4.4582e-02,  1.4308e-02,  2.9567e-02,  1.0341e-02,\n",
       "           -2.5082e-03, -2.4567e-02, -5.3316e-04,  4.0768e-02,  1.3410e-02,\n",
       "           -2.8722e-02, -1.4921e-02,  7.6955e-03, -7.2807e-03, -4.8398e-03,\n",
       "           -1.5638e-02, -3.2571e-03,  1.0078e-02,  8.8877e-03, -1.0616e-02,\n",
       "            1.9201e-04,  4.7869e-03,  7.6771e-03, -2.0228e-02,  1.9961e-04,\n",
       "            2.9737e-02,  1.5060e-03,  3.0772e-03,  4.9462e-02, -2.0164e-02,\n",
       "            7.2670e-03,  4.7180e-03,  3.2135e-02,  2.3778e-03, -1.5088e-02,\n",
       "           -4.6102e-04, -1.0990e-02, -2.0417e-02,  1.1567e-03,  2.3239e-02,\n",
       "            1.8093e-04, -1.3742e-03,  9.9396e-03,  1.5547e-02,  1.1223e-04,\n",
       "            1.4432e-02, -1.6222e-03, -1.2030e-02,  4.1439e-03,  2.2759e-02,\n",
       "           -8.8496e-04, -5.2928e-02,  7.8545e-03, -1.0793e-03,  3.6379e-04,\n",
       "            1.2993e-02, -6.6117e-03, -3.0101e-03,  3.3173e-04, -2.5878e-02,\n",
       "           -7.0100e-03, -6.0765e-03, -5.6908e-02, -6.9117e-02, -5.9733e-03,\n",
       "           -7.1765e-03,  2.8758e-02, -7.0431e-04, -4.7443e-03, -2.2998e-02,\n",
       "           -7.7887e-02, -1.7900e-03,  2.2912e-03,  5.4989e-03, -1.0063e-02,\n",
       "           -2.1349e-02,  2.5279e-03, -3.9801e-03, -1.4251e-02,  1.5382e-03]],\n",
       " \n",
       "         [[-2.3286e-03,  2.0283e-03, -6.6744e-04, -1.6559e-02,  6.5720e-03,\n",
       "           -1.2079e-02,  4.9629e-02,  2.5286e-04,  2.2596e-04,  2.7547e-02,\n",
       "            5.7574e-03,  5.1312e-02, -6.2244e-03, -4.7173e-03,  2.3380e-02,\n",
       "           -9.4676e-02,  1.9332e-02, -8.1813e-03,  1.0420e-02, -1.2491e-02,\n",
       "           -5.4599e-03,  3.1452e-03,  1.4201e-03, -1.2396e-02,  1.2030e-03,\n",
       "            1.2347e-02,  4.4215e-03,  4.1918e-03,  2.9093e-03,  1.3054e-04,\n",
       "           -2.4323e-02,  8.2645e-03, -5.1275e-03,  1.6533e-02,  2.5841e-02,\n",
       "           -2.3577e-02,  1.5053e-02, -3.2556e-02,  1.4776e-02,  2.1859e-02,\n",
       "            1.4961e-03,  2.6019e-03,  2.0043e-03, -7.3157e-03,  2.5654e-02,\n",
       "            2.0396e-02, -2.8950e-02, -1.4456e-02,  8.1608e-03,  7.9425e-04,\n",
       "           -6.1699e-03,  1.5219e-02, -4.3468e-05, -5.1164e-02,  3.3460e-04,\n",
       "            7.7237e-03, -8.1609e-03,  3.0864e-04,  1.5822e-02,  5.9293e-03,\n",
       "           -2.6189e-03,  5.3244e-03,  1.0650e-02,  8.5100e-03,  1.9794e-02,\n",
       "            4.8995e-03, -1.1987e-02,  2.1339e-02,  4.5367e-05, -1.5962e-02,\n",
       "            3.0456e-03, -4.0318e-03,  2.5835e-03, -3.8167e-03,  3.0582e-02,\n",
       "            1.1173e-03,  1.8132e-02,  1.5550e-02,  2.2462e-02,  2.5909e-03,\n",
       "           -2.1253e-02,  1.6961e-02,  8.3556e-03, -1.1336e-02, -3.3049e-02,\n",
       "            3.3460e-02,  2.2388e-03,  4.5869e-02, -1.3988e-02, -5.7786e-03,\n",
       "            1.1054e-03, -8.2625e-03, -5.8029e-04,  6.2101e-03, -3.8918e-03,\n",
       "           -3.8307e-02, -4.3026e-03,  6.4529e-04, -1.1102e-02, -1.6856e-03,\n",
       "            1.3104e-02,  1.0549e-02,  5.8109e-03,  2.9451e-03, -1.8629e-02,\n",
       "            1.9396e-03, -1.6939e-03,  8.8995e-03, -1.5541e-02,  3.9094e-02,\n",
       "           -3.7681e-03,  2.6657e-02,  1.0847e-04,  1.5947e-02, -3.4866e-02,\n",
       "            7.6270e-02, -9.3866e-03,  3.8811e-04, -2.4564e-03,  4.2912e-02,\n",
       "           -1.6110e-02,  4.3428e-02, -1.9180e-03,  2.2523e-04, -1.1059e-04,\n",
       "           -1.0873e-02, -7.2969e-03,  9.6337e-04,  3.9888e-03, -2.0258e-05,\n",
       "            7.2102e-03,  1.5817e-02, -5.9354e-02, -8.2039e-04, -7.7920e-03,\n",
       "            5.5584e-03,  2.5050e-04,  1.6252e-03, -2.4397e-02, -2.8518e-03,\n",
       "           -2.2400e-02, -4.3824e-03,  3.9032e-03,  8.9161e-03, -3.5756e-04,\n",
       "           -2.1744e-03, -9.7966e-04,  1.7553e-02,  7.4509e-03,  4.4980e-02,\n",
       "           -3.6480e-03,  2.4134e-03,  4.5234e-03,  8.6861e-03,  2.0400e-02,\n",
       "           -4.3157e-05, -1.6100e-03, -2.1848e-03, -2.2111e-03,  1.4692e-02,\n",
       "            2.4300e-02,  3.3802e-03,  2.3725e-02,  4.8159e-03, -1.1157e-02,\n",
       "           -3.8345e-03,  6.5152e-03,  5.9454e-03, -4.0439e-02,  4.6724e-03,\n",
       "           -6.1756e-03,  1.6890e-02,  1.1168e-02, -9.9416e-04,  2.0177e-02,\n",
       "           -1.3920e-03,  9.9222e-03,  6.8456e-03,  3.6914e-03,  1.6102e-03,\n",
       "            3.3946e-03,  1.1744e-03, -7.5108e-03,  5.4903e-02, -1.2558e-02,\n",
       "            1.6928e-02,  2.2004e-03, -1.8285e-03, -1.1653e-02, -1.1406e-02,\n",
       "           -2.6765e-02,  2.5559e-03, -6.5925e-03, -2.6651e-03, -1.6569e-02,\n",
       "           -5.8750e-02,  2.6631e-02,  1.4833e-02,  1.7115e-03,  2.6244e-03]]],\n",
       "        device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([-0.0020, -0.0051,  0.0061], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "ig.attribute(embs, ref_emb, return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(model, interpret_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/narine/debug_squad' # <PATH-TO-SAVED-MODEL>\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    return model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask)\n",
    "    \n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "        pred_ = predict(inputs,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        position_ids=position_ids,\n",
    "                        attention_mask=attention_mask)[position]\n",
    "        return pred_.max(1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"[PAD]\"\n",
    "sep = \"[SEP]\"\n",
    "cls = \"[CLS]\"\n",
    "\n",
    "ref_token_id = tokenizer.encode(ref)\n",
    "sep_token_id = tokenizer.encode(sep)\n",
    "cls_token_id = tokenizer.encode(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question)\n",
    "    text_ids = tokenizer.encode(text)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = cls_token_id + question_ids + sep_token_id + text_ids + sep_token_id\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = cls_token_id + ref_token_id * len(question_ids) + sep_token_id + \\\n",
    "        ref_token_id * len(text_ids) + sep_token_id\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    input_ids_ = input_ids[0]\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(len(input_ids_))]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids)\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = torch.zeros_like(position_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"How many artworks did Van Gogh create?\", \\\n",
    "                 \"In just over a decade Vincent van Gogh created about 2,100 artworks, \" \\\n",
    "                 \"including around 860 oil paintings, most of which date from the last two years of his life.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 tensor([[  101,  1293,  1242, 25466,  1225,  3498,  1301,  5084,  2561,   136,\n",
      "           102,  1107,  1198,  1166,   170,  4967,   191,  1394,  8298,  3498,\n",
      "          1301,  5084,  1687,  1164,   123,   117,  1620, 25466,   117,  1259,\n",
      "          1213,  5942,  1568,  2949,  4694,   117,  1211,  1104,  1134,  2236,\n",
      "          1121,  1103,  1314,  1160,  1201,  1104,  1117,  1297,   119,   102]],\n",
      "       device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]],\n",
      "       device='cuda:0') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "print(input_ids.dtype, input_ids)\n",
    "print(token_type_ids, ref_token_type_ids)\n",
    "print(position_ids, ref_position_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad_pos_forward_func:  tensor([6.8525], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "output.shape:  torch.Size([50])\n",
      "output.shape:  torch.Size([50])\n",
      "output.shape:  torch.Size([50])\n",
      "output.shape:  torch.Size([50])\n",
      "output.shape:  torch.Size([50])\n",
      "output.shape:  torch.Size([50])\n",
      "attributions_start:  [ 0.          0.54717004  0.3882186  -0.03181905  0.0175079   0.07919758\n",
      "  0.15983962 -0.00674258  0.08660728  0.21139494 -0.07965035  0.0599708\n",
      " -0.00141058  0.0131349   0.10260752 -0.02915622  0.01871186  0.06346903\n",
      "  0.00851475 -0.06987926  0.00704876 -0.0222609   0.02858604  0.12898968\n",
      "  0.30991644  0.0919048   0.2707097  -0.02806081  0.02801514  0.02262776\n",
      "  0.03283522 -0.0422463  -0.00726324  0.00105244 -0.02436556  0.01064256\n",
      " -0.02947477  0.06351329  0.10911785 -0.08753464  0.05641276  0.43600887\n",
      "  0.03995525  0.02896365 -0.0030782   0.08050218 -0.00569606 -0.05158571\n",
      "  0.0208116  -0.04891273]\n",
      "attributions_end:  [ 0.          0.59831417  0.48700386  0.08439662 -0.01518923  0.08282123\n",
      "  0.20406719 -0.02219354  0.09927205  0.24198017  0.00215905  0.05058072\n",
      " -0.01104547  0.00864966  0.1086959   0.00510568 -0.00321924  0.0497947\n",
      " -0.00623684 -0.12937032  0.01454291 -0.05014657 -0.02288465  0.06177828\n",
      "  0.07128762  0.00343875  0.1275599   0.04463881  0.0367193   0.03188661\n",
      "  0.08566057  0.0224555   0.16727646  0.20495875  0.21836138  0.04265161\n",
      " -0.00309139  0.06599117  0.14129235 -0.08113281  0.0617153   0.14461827\n",
      " -0.01094523  0.14231881  0.02386383  0.0598155  -0.01730959  0.05395661\n",
      "  0.05017098  0.01475127]\n",
      "['[CLS]', 'how', 'many', 'artworks', 'did', 'van', 'go', '##gh', 'create', '?', '[SEP]', 'in', 'just', 'over', 'a', 'decade', 'v', '##in', '##cent', 'van', 'go', '##gh', 'created', 'about', '2', ',', '100', 'artworks', ',', 'including', 'around', '86', '##0', 'oil', 'paintings', ',', 'most', 'of', 'which', 'date', 'from', 'the', 'last', 'two', 'years', 'of', 'his', 'life', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    return attributions\n",
    "\n",
    "lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "#nt = NoiseTunnel(ig)\n",
    "\n",
    "print('squad_pos_forward_func: ', squad_pos_forward_func(input_ids, token_type_ids, attention_mask=attention_mask))\n",
    "model.zero_grad()\n",
    "attributions_start = lig.attribute(inputs=(input_ids, token_type_ids, position_ids), baselines=(ref_input_ids, ref_token_type_ids, ref_position_ids), additional_forward_args=(attention_mask, 0))\n",
    "attributions_end = lig.attribute(inputs=(input_ids, token_type_ids, position_ids), baselines=(ref_input_ids, ref_token_type_ids, ref_position_ids), additional_forward_args=(attention_mask, 1))\n",
    "\n",
    "attributions_start_ = summarize_attributions(attributions_start[0])\n",
    "attributions_end_ = summarize_attributions(attributions_end[0])\n",
    "\n",
    "print('attributions_start: ', attributions_start_)\n",
    "print('attributions_end: ', attributions_end_)\n",
    "text = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu().numpy())\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>Target Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>0.0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>2.95</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artworks                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> van                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> go                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##gh                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> create                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> just                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> over                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decade                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> v                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##cent                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> van                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> go                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##gh                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> created                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 100                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artworks                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> including                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> around                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 86                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oil                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paintings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> date                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> last                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> years                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>0.0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>3.54</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artworks                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> van                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> go                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##gh                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> create                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> just                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> over                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decade                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> v                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##cent                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> van                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> go                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##gh                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> created                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 100                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artworks                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> including                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> around                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 86                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oil                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paintings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> date                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> last                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> years                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_data_records = []\n",
    "# storing couple samples in an array for visualization purposes\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_start_,\n",
    "                        0.0, #torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        0.0, #torch.argmax(start_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_start_.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_end_,\n",
    "                        0.0, #torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        0.0, #torch.argmax(end_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_end_.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "viz.visualize_text(vis_data_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
