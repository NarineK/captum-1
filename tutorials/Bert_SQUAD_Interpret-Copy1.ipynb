{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting BERT Models (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to interpret Bert models using  `Captum` library. In this particular case study we focus on a fine-tuned Question Answering model on SQUAD dataset using transformers library from Hugging Face: https://huggingface.co/transformers/\n",
    "\n",
    "We show how to use interpretation hooks to examine and better understand embedding, bert, and attention layers. \n",
    "\n",
    "Note: Before running this tutorial, please install the seaborn and matplotlib python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to fine tune BERT model on SQUAD dataset. This can be easiy accomplished by following the steps described in hugging face's official web site: https://github.com/huggingface/transformers#run_squadpy-fine-tuning-on-squad-for-question-answering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we pretrained the model, we can loading the tokenizer and pre-trained BERT model using commands described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/narine/debug_squad' # <PATH-TO-SAVED-MODEL>\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path) #, output_attentions=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe0a39e90b0>"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to perform forward pass of the model and accomplish predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    return model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom forward function that will allow us to access the start and end postitions of our prediction using `position` input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret parent `BertEmbeddings` layer. For that reason we separate embedding layer and precompute the embeddings. `configure_interpretable_embedding_layer` function helps us to do that. It returns `InterpretableEmbeddingBase` layer that wraps `BertEmbeddings` and can be used to access the embedding vectors. Note that we need to remove `InterpretableEmbeddingBase` wrapper from our model using `remove_interpretable_embedding_layer` function after we finish interpretation.\n",
    "\n",
    "Note that there is a way to interpret BertEmbeddings without configuring interpretable embedding layers that we will make available soon, However it is important to not that configuring interpretable embedding layers gives us more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/narine/captum/captum/attr/_models/base.py:181: UserWarning: In order to make embedding layers more interpretable they will\n",
      "        be replaced with an interpretable embedding layer which wraps the\n",
      "        original embedding layer and takes word embedding vectors as inputs of\n",
      "        the forward function. This allows to generate baselines for word\n",
      "        embeddings and compute attributions for each embedding dimension.\n",
      "        The original embedding layer must be set\n",
      "        back by calling `remove_interpretable_embedding_layer` function\n",
      "        after model interpretation is finished.\n",
      "  after model interpretation is finished.\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): InterpretableEmbeddingBase(\n",
       "      (embedding): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and numericalizing / encoding special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"[PAD]\" # A token used for generating token reference\n",
    "sep = \"[SEP]\" # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls = \"[CLS]\" # This token is prepended to the concatenated question-text word sequence\n",
    "\n",
    "ref_token_id = tokenizer.encode(ref)\n",
    "sep_token_id = tokenizer.encode(sep)\n",
    "cls_token_id = tokenizer.encode(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a set of helper function fo constructing refenreces / baselines for word tokens, token types and position ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question)\n",
    "    text_ids = tokenizer.encode(text)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = cls_token_id + question_ids + sep_token_id + text_ids + sep_token_id\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = cls_token_id + ref_token_id * len(question_ids) + sep_token_id + \\\n",
    "        ref_token_id * len(text_ids) + sep_token_id\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    input_ids_ = input_ids[0]\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(len(input_ids_))]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    ref_position_ids = position_ids[torch.randperm(seq_length, device=device)]\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_sub_bert_embedding(input_ids, ref_input_ids,\n",
    "                                   token_type_ids, ref_token_type_ids,\n",
    "                                   position_ids, ref_position_ids):\n",
    "    input_embeddings = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding1.indices_to_embeddings(ref_input_ids)\n",
    "\n",
    "    input_embeddings_token_type = interpretable_embedding2.indices_to_embeddings(token_type_ids)\n",
    "    ref_input_embeddings_token_type = interpretable_embedding2.indices_to_embeddings(ref_token_type_ids)\n",
    "\n",
    "    input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(position_ids)\n",
    "    ref_input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(ref_position_ids)\n",
    "    \n",
    "    return (input_embeddings, ref_input_embeddings), \\\n",
    "           (input_embeddings_token_type, ref_input_embeddings_token_type), \\\n",
    "           (input_embeddings_position_ids, ref_input_embeddings_position_ids)\n",
    "    \n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the `question - text` pair that we'd like to use as an input for our Bert model and interpret what the model was forcusing on when predicting an answer to the question from given input text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "#question, text = \"How old is Jim Henson?\", \"Jim Henson is 24 years old\"\n",
    "\n",
    "#question, text = \"How many artworks did Van Gogh create?\", \\\n",
    "#                 \"In just over a decade Vincent van Gogh created about 2,100 artworks, \" \\\n",
    "#                 \"including around 860 oil paintings, most of which date from the last two years of his life.\"\n",
    "\n",
    "question, text = \"What is important to us?\", \"It is important to us to include, empower and support humans of all kinds.\"\n",
    "#question, text = \"Who is most likely to teach a child at home?\", \"In some countries, formal education can take place through home schooling. Informal learning may be assisted by a teacher occupying a transient or ongoing role, such as a family member, or by anyone with knowledge or skills in the wider community setting.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's numericalize the question, the input text and generate corresponding references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 tensor([[ 101, 1184, 1110, 1696, 1106, 1366,  136,  102, 1122, 1110, 1696, 1106,\n",
      "         1366, 1106, 1511,  117, 9712, 9447, 1105, 1619, 3612, 1104, 1155, 7553,\n",
      "          119,  102]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25]], device='cuda:0') tensor([[25, 23,  5,  3,  8, 20,  0,  1, 10, 12, 22,  6, 21, 11, 13,  7,  2, 14,\n",
      "         19, 16, 17, 18,  4, 24, 15,  9]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "print(input_ids.dtype, input_ids)\n",
    "print(token_type_ids, ref_token_type_ids)\n",
    "print(position_ids, ref_position_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's comput the embedding vectors for BertEmbedding and make predictions using those embeddings both for the input and reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to include , em ##power and support humans of all kinds\n",
      "it is important to us to include , em ##power and support humans of all kinds .\n",
      "tensor([[[ 0.4368,  0.0743, -0.2138,  ...,  0.0359,  0.0735, -0.1483],\n",
      "         [-0.7860, -1.1014,  0.0221,  ..., -0.4431, -0.5254,  0.4514],\n",
      "         [-1.1792,  0.2095,  0.6804,  ...,  0.5314,  1.1256,  0.7433],\n",
      "         ...,\n",
      "         [ 0.0305, -0.0719, -0.5761,  ..., -1.1417, -1.6071,  1.0821],\n",
      "         [-0.2231,  0.3109,  1.0365,  ...,  0.1760,  0.3919,  0.3608],\n",
      "         [-0.0254, -0.5551,  0.4981,  ...,  0.4892, -0.6702,  0.6614]]],\n",
      "       device='cuda:0', grad_fn=<AddcmulBackward>) tensor([[[ 0.4368,  0.0743, -0.2138,  ...,  0.0359,  0.0735, -0.1483],\n",
      "         [-0.4586, -0.8524,  0.3253,  ...,  0.3703, -0.4630,  0.4020],\n",
      "         [-0.1351, -0.9157,  0.5216,  ...,  0.3649, -0.5518,  0.2779],\n",
      "         ...,\n",
      "         [ 0.6944, -1.3654,  0.6246,  ...,  0.2864, -0.3174,  0.8840],\n",
      "         [ 0.5152, -1.0316,  0.5615,  ...,  0.3071, -0.4280,  0.5677],\n",
      "         [-0.0254, -0.5551,  0.4981,  ...,  0.4892, -0.6702,  0.6614]]],\n",
      "       device='cuda:0', grad_fn=<AddcmulBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)\n",
    "\n",
    "start_scores, end_scores = predict(input_embeddings, \\\n",
    "                                   token_type_ids=token_type_ids, \\\n",
    "                                   position_ids=position_ids, \\\n",
    "                                   attention_mask=attention_mask)\n",
    "\n",
    "ref_start_scores, ref_end_scores = predict(ref_input_embeddings, \\\n",
    "                                           token_type_ids=ref_token_type_ids, \\\n",
    "                                           position_ids=ref_position_ids, \\\n",
    "                                           attention_mask=attention_mask)\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].detach().tolist()) \n",
    "print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "print(' '.join(all_tokens[torch.argmax(ref_start_scores) : torch.argmax(ref_end_scores)+1]))\n",
    "print(input_embeddings, ref_input_embeddings)\n",
    "#print(attn[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squad_pos_forward_func(input_embeddings, token_type_ids, position_ids, attention_mask)\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    #attributions = attributions.cpu().detach().numpy()\n",
    "    return attributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'is', 'important', 'to', 'us', '?', '[SEP]', 'it', 'is', 'important', 'to', 'us', 'to', 'include', ',', 'em', '##power', 'and', 'support', 'humans', 'of', 'all', 'kinds', '.', '[SEP]'] [CLS] what is important to us ? [SEP] it is important to us to include , em ##power and support humans of all kinds . [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu().numpy())\n",
    "print(text, ' '.join(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad_pos_forward_func:  tensor([4.7419], device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "attributions_start_sum:  tensor([ 0.0000,  0.3672, -0.0884,  0.5862,  0.0149,  0.2525,  0.4260,  0.0000,\n",
      "        -0.2238,  0.2537,  0.0558,  0.0308,  0.1517,  0.0672, -0.3056,  0.0616,\n",
      "        -0.0136, -0.0953, -0.0646, -0.0673, -0.0361, -0.0371,  0.0155,  0.0612,\n",
      "         0.0223,  0.0000], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "attributions_end_sum:  tensor([ 0.0000,  0.5829, -0.0204,  0.1322, -0.1351,  0.1766,  0.5436,  0.0000,\n",
      "        -0.0714,  0.1992,  0.0549,  0.0215,  0.3113,  0.0719,  0.1828, -0.0160,\n",
      "        -0.0449, -0.1184,  0.0256, -0.0380, -0.1886,  0.0202,  0.0161,  0.2358,\n",
      "         0.0592,  0.0000], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ig = IntegratedGradients(squad_pos_forward_func)\n",
    "\n",
    "#nt = NoiseTunnel(ig)\n",
    "\n",
    "print('squad_pos_forward_func: ', squad_pos_forward_func(input_embeddings, token_type_ids, attention_mask=attention_mask))\n",
    "model.zero_grad()\n",
    "attributions_start = ig.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(token_type_ids, position_ids,attention_mask, 0))\n",
    "attributions_end = ig.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(token_type_ids, position_ids, attention_mask, 1))\n",
    "attributions_start_sum = summarize_attributions(attributions_start)\n",
    "attributions_end_sum = summarize_attributions(attributions_end)\n",
    "\n",
    "print('attributions_start_sum: ', attributions_start_sum)\n",
    "print('attributions_end_sum: ', attributions_end_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.convert_ids_to_tokens(input_ids[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>Target Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>13 (0.72)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>1.43</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>23 (0.73)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>2.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_data_records = []\n",
    "# storing couple samples in an array for visualization purposes\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_start_sum,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_start_sum.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_end_sum,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_end_sum.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "viz.visualize_text(vis_data_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top words in start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_attributed_tokens(attrs, k=5):\n",
    "    values, indices = torch.topk(attrs, k)\n",
    "    top_tokens = [text[idx] for idx in indices]\n",
    "    return top_tokens, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k start tokens:  ['important', '?', 'what', 'is', 'us'] tensor([3, 6, 1, 9, 5], device='cuda:0')\n",
      "Top k end tokens:  ['what', '?', 'us', 'kinds', 'is'] tensor([ 1,  6, 12, 23,  9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_tokens_start, indices_start = get_topk_attributed_tokens(attributions_start_sum)\n",
    "top_tokens_end, indices_end = get_topk_attributed_tokens(attributions_end_sum)\n",
    "\n",
    "print('Top k start tokens: ', top_tokens_start, indices_start)\n",
    "print('Top k end tokens: ', top_tokens_end, indices_end)\n",
    "# TODO put this in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.6003, -5.0592, -7.5001, -7.0007, -7.3166, -7.2239, -8.5239, -3.4123,\n",
       "           1.8587, -3.7432, -0.5150, -3.6213, -0.4382,  4.7419,  2.9291, -2.8921,\n",
       "           0.7817, -3.3601, -4.6933,  1.3297,  2.4032, -1.6884, -1.3988, -0.7250,\n",
       "          -3.1820, -3.4123]], device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-6.4740, -5.9008, -9.1457, -8.0400, -8.5174, -7.5224, -8.5123, -3.3396,\n",
       "          -4.0507, -7.2815, -3.6809, -6.9168, -2.7833, -3.1995, -0.2528, -3.3636,\n",
       "          -5.8645,  0.0294, -5.1065,  0.1928,  4.7798, -2.1166, -1.9008,  5.9077,\n",
       "           2.6237, -3.3396]], device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores, end_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_interpretable_embedding_layer(model, interpretable_embedding)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Embedding attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): InterpretableEmbeddingBase(\n",
       "        (embedding): Embedding(28996, 768, padding_idx=0)\n",
       "      )\n",
       "      (position_embeddings): InterpretableEmbeddingBase(\n",
       "        (embedding): Embedding(512, 768)\n",
       "      )\n",
       "      (token_type_embeddings): InterpretableEmbeddingBase(\n",
       "        (embedding): Embedding(2, 768)\n",
       "      )\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpretable_embedding1 = configure_interpretable_embedding_layer(model, 'bert.embeddings.word_embeddings')\n",
    "interpretable_embedding2 = configure_interpretable_embedding_layer(model, 'bert.embeddings.token_type_embeddings')\n",
    "interpretable_embedding3 = configure_interpretable_embedding_layer(model, 'bert.embeddings.position_embeddings')\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting each embedding layer within `BertEmbeddings`, `word_embeddings`, `token_type_embeddings` and `position_embeddings` in a separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0324,  0.0058, -0.0033,  ...,  0.0032, -0.0071, -0.0188],\n",
      "         [-0.0186, -0.0628,  0.0058,  ..., -0.0394, -0.0393,  0.0298],\n",
      "         [-0.0480,  0.0170,  0.0330,  ...,  0.0157,  0.0569,  0.0507],\n",
      "         ...,\n",
      "         [-0.0304,  0.0146, -0.0477,  ..., -0.0813, -0.1123,  0.0360],\n",
      "         [-0.0259,  0.0305,  0.0514,  ...,  0.0010,  0.0147,  0.0187],\n",
      "         [-0.0036, -0.0044,  0.0180,  ...,  0.0135, -0.0429,  0.0281]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward>) tensor([[[ 0.0324,  0.0058, -0.0033,  ...,  0.0032, -0.0071, -0.0188],\n",
      "         [ 0.0004, -0.0428,  0.0129,  ..., -0.0035, -0.0345,  0.0162],\n",
      "         [ 0.0004, -0.0428,  0.0129,  ..., -0.0035, -0.0345,  0.0162],\n",
      "         ...,\n",
      "         [ 0.0004, -0.0428,  0.0129,  ..., -0.0035, -0.0345,  0.0162],\n",
      "         [ 0.0004, -0.0428,  0.0129,  ..., -0.0035, -0.0345,  0.0162],\n",
      "         [-0.0036, -0.0044,  0.0180,  ...,  0.0135, -0.0429,  0.0281]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward>)\n",
      "to include , em ##power and support humans of all kinds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(input_embed, ref_input_embed), (token_type_ids_embed, ref_token_type_ids_embed), (position_ids_embed, ref_position_ids_embed)  = construct_sub_bert_embedding(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)\n",
    "print(input_embed, ref_input_embed)\n",
    "start_scores, end_scores = predict(input_embed, \\\n",
    "                                   token_type_ids=token_type_ids_embed, \\\n",
    "                                   position_ids=position_ids_embed, \\\n",
    "                                   attention_mask=attention_mask)\n",
    "ref_start_scores, ref_end_scores = predict(ref_input_embed, \\\n",
    "                                           token_type_ids=ref_token_type_ids_embed, \\\n",
    "                                           position_ids=ref_position_ids_embed, \\\n",
    "                                           attention_mask=attention_mask)\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].detach().tolist()) \n",
    "print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "print(' '.join(all_tokens[torch.argmax(ref_start_scores) : torch.argmax(ref_end_scores)+1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0056, -0.0193,  0.0119,  ...,  0.0148,  0.0095,  0.0090],\n",
      "         [ 0.0240, -0.0254,  0.0099,  ...,  0.0100,  0.0106,  0.0175],\n",
      "         [-0.0066,  0.0184, -0.0078,  ...,  0.0032,  0.0068, -0.0232],\n",
      "         ...,\n",
      "         [ 0.0164, -0.0100,  0.0076,  ...,  0.0114,  0.0058,  0.0037],\n",
      "         [ 0.0009, -0.0088,  0.0009,  ...,  0.0032,  0.0175, -0.0037],\n",
      "         [ 0.0071,  0.0080,  0.0015,  ...,  0.0109,  0.0187, -0.0039]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward>) tensor([[[ 0.0224,  0.0048, -0.0132,  ...,  0.0004,  0.0156,  0.0074],\n",
      "         [-0.0270, -0.0053, -0.0055,  ...,  0.0065,  0.0055, -0.0064],\n",
      "         [-0.0125, -0.0082,  0.0037,  ...,  0.0069,  0.0017, -0.0113],\n",
      "         ...,\n",
      "         [ 0.0240, -0.0254,  0.0099,  ...,  0.0100,  0.0106,  0.0175],\n",
      "         [ 0.0164, -0.0100,  0.0076,  ...,  0.0114,  0.0058,  0.0037],\n",
      "         [ 0.0056, -0.0193,  0.0119,  ...,  0.0148,  0.0095,  0.0090]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward>)\n",
      "attributions_start:  tensor([ 0.0000,  0.1758,  0.3012,  0.0774, -0.1777, -0.1195,  0.0982,  0.0000,\n",
      "         0.4738,  0.1192,  0.0596,  0.0951, -0.0083,  0.0802,  0.1656,  0.3834,\n",
      "         0.1447,  0.0862,  0.0118, -0.1512,  0.4241, -0.1684,  0.3489, -0.0515,\n",
      "         0.0287,  0.0000], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "attributions_end:  tensor([ 0.0000,  0.0990,  0.1941,  0.0096,  0.5199, -0.2718,  0.0369,  0.0000,\n",
      "         0.1015,  0.0455, -0.1553,  0.2550, -0.0768,  0.1348,  0.1515,  0.2641,\n",
      "        -0.0638, -0.0119,  0.3319,  0.1227,  0.1511, -0.2406,  0.0074,  0.2693,\n",
      "        -0.3254,  0.0000], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "attributions_start:  tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0885,\n",
      "         0.3465,  0.1394,  0.2502,  0.2046,  0.3452,  0.4571,  0.1475,  0.2409,\n",
      "         0.2119, -0.1263,  0.2682,  0.0468,  0.2150,  0.2471,  0.0492,  0.2006,\n",
      "         0.2183,  0.0882], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "attributions_end:  tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0190,\n",
      "         0.0521,  0.0254,  0.0245,  0.0253, -0.0255, -0.0010, -0.0412, -0.0081,\n",
      "         0.0349,  0.0601,  0.1642,  0.0990,  0.9303,  0.1928,  0.0606,  0.1872,\n",
      "         0.0890,  0.0428], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "attributions_start:  tensor([ 0.0112, -0.3024,  0.0245,  0.0000,  0.1074,  0.0509,  0.6267, -0.0319,\n",
      "         0.0291, -0.3220, -0.3228, -0.1504, -0.3217, -0.0163, -0.0610,  0.0196,\n",
      "         0.3068,  0.0335,  0.1177,  0.0300, -0.0090,  0.0466, -0.1297, -0.0211,\n",
      "         0.1421, -0.1047], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "attributions_end:  tensor([-1.0420e-02, -6.5909e-01,  4.3261e-02,  0.0000e+00, -1.8562e-01,\n",
      "         1.1633e-01,  5.9470e-01,  8.7436e-02,  1.2274e-03,  7.5037e-02,\n",
      "        -9.9077e-02, -1.5153e-01, -3.0112e-02,  2.5011e-03, -1.9461e-04,\n",
      "        -2.6467e-01,  1.3050e-02,  6.0535e-02,  3.1036e-02, -5.6558e-02,\n",
      "        -1.0577e-02,  4.1056e-02, -1.6522e-02, -1.6452e-02,  9.4404e-02,\n",
      "        -1.6030e-01], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(ref_position_ids_embed, position_ids_embed)\n",
    "attributions_start = ig.attribute(inputs=(input_embed, token_type_ids_embed, position_ids_embed),\n",
    "                                  baselines=(ref_input_embed, ref_token_type_ids_embed, ref_position_ids_embed),\n",
    "                                  additional_forward_args=(attention_mask, 0))\n",
    "attributions_end = ig.attribute(inputs=(input_embed, token_type_ids_embed, position_ids_embed),\n",
    "                                  baselines=(ref_input_embed, ref_token_type_ids_embed, ref_position_ids_embed),\n",
    "                                  additional_forward_args=(attention_mask, 1))\n",
    "\n",
    "attributions_start_1 = summarize_attributions(attributions_start[0])\n",
    "attributions_end_1 = summarize_attributions(attributions_end[0])\n",
    "\n",
    "print('attributions_start: ', attributions_start_1)\n",
    "print('attributions_end: ', attributions_end_1)\n",
    "\n",
    "attributions_start_2 = summarize_attributions(attributions_start[1])\n",
    "attributions_end_2 = summarize_attributions(attributions_end[1])\n",
    "\n",
    "print('attributions_start: ', attributions_start_2)\n",
    "print('attributions_end: ', attributions_end_2)\n",
    "\n",
    "\n",
    "attributions_start_3 = summarize_attributions(attributions_start[2])\n",
    "attributions_end_3 = summarize_attributions(attributions_end[2])\n",
    "\n",
    "print('attributions_start: ', attributions_start_3)\n",
    "print('attributions_end: ', attributions_end_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>Target Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>13 (0.72)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>2.40</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>23 (0.73)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>1.55</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>13 (0.72)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>3.64</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>23 (0.73)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>1.93</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 54%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>13 (0.72)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>-0.25</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>23 (0.73)</b></text></td><td><text style=\"padding-right:2em\"><b>2100</b></text></td><td><text style=\"padding-right:2em\"><b>-0.50</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_data_records = []\n",
    "# storing couple samples in an array for visualization purposes\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_start_1,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_start_1.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_end_1,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_end_1.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_start_2,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_start_2.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_end_2,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_end_2.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_start_3,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_start_3.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                        attributions_end_3,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        '2100',\n",
    "                        '2100',\n",
    "                        attributions_end_3.sum(),       \n",
    "                        text,\n",
    "                        -1.0))\n",
    "\n",
    "viz.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(model, interpretable_embedding1)\n",
    "remove_interpretable_embedding_layer(model, interpretable_embedding2)\n",
    "remove_interpretable_embedding_layer(model, interpretable_embedding3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k start tokens:  ['it', 'humans', ',', 'all', 'is'] tensor([ 8, 20, 15, 22,  2], device='cuda:0')\n",
      "Top k end tokens:  ['to', 'and', 'kinds', ',', 'to'] tensor([ 4, 18, 23, 15, 11], device='cuda:0')\n",
      "Top k start token types:  ['to', 'it', 'us', 'and', 'important'] tensor([13,  8, 12, 18, 10], device='cuda:0')\n",
      "Top k end token types:  ['humans', 'of', 'kinds', 'and', 'support'] tensor([20, 21, 23, 18, 19], device='cuda:0')\n",
      "Top k start position id:  ['?', 'em', '.', 'and', 'to'] tensor([ 6, 16, 24, 18,  4], device='cuda:0')\n",
      "Top k end position id:  ['?', 'us', '.', '[SEP]', 'is'] tensor([ 6,  5, 24,  7,  9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "top_tokens_start, indices_start = get_topk_attributed_tokens(attributions_start_1)\n",
    "top_tokens_end, indices_end = get_topk_attributed_tokens(attributions_end_1)\n",
    "\n",
    "print('Top k start tokens: ', top_tokens_start, indices_start)\n",
    "print('Top k end tokens: ', top_tokens_end, indices_end)\n",
    "\n",
    "top_tokens_start, indices_start = get_topk_attributed_tokens(attributions_start_2)\n",
    "top_tokens_end, indices_end = get_topk_attributed_tokens(attributions_end_2)\n",
    "\n",
    "print('Top k start token types: ', top_tokens_start, indices_start)\n",
    "print('Top k end token types: ', top_tokens_end, indices_end)\n",
    "\n",
    "top_tokens_start, indices_start = get_topk_attributed_tokens(attributions_start_3)\n",
    "top_tokens_end, indices_end = get_topk_attributed_tokens(attributions_end_3)\n",
    "\n",
    "print('Top k start position id: ', top_tokens_start, indices_start)\n",
    "print('Top k end position id: ', top_tokens_end, indices_end)\n",
    "\n",
    "# Put this in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class BertForQuestionAnswering(BertPreTrainedModel):\\n    r\"\"\"\\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\\n            Positions are clamped to the length of the sequence (`sequence_length`).\\n            Position outside of the sequence are not taken into account for computing the loss.\\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\\n            Positions are clamped to the length of the sequence (`sequence_length`).\\n            Position outside of the sequence are not taken into account for computing the loss.\\n\\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\\n            Span-start scores (before SoftMax).\\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\\n            Span-end scores (before SoftMax).\\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\\n            of shape ``(batch_size, sequence_length, hidden_size)``:\\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\\n\\n    Examples::\\n\\n        tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\n        model = BertForQuestionAnswering.from_pretrained(\\'bert-base-uncased\\')\\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\\n        start_positions = torch.tensor([1])\\n        end_positions = torch.tensor([3])\\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\\n        loss, start_scores, end_scores = outputs[:2]\\n\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertForQuestionAnswering, self).__init__(config)\\n        self.num_labels = config.num_labels\\n\\n        self.bert = BertModel(config)\\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\\n\\n        self.init_weights()\\n\\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\\n                start_positions=None, end_positions=None):\\n\\n        outputs = self.bert(input_ids,\\n                            attention_mask=attention_mask,\\n                            token_type_ids=token_type_ids,\\n                            position_ids=position_ids,\\n                            head_mask=head_mask)\\n\\n        sequence_output = outputs[0]\\n\\n        logits = self.qa_outputs(sequence_output)\\n        start_logits, end_logits = logits.split(1, dim=-1)\\n        start_logits = start_logits.squeeze(-1)\\n        end_logits = end_logits.squeeze(-1)\\n\\n        outputs = (start_logits, end_logits,) + outputs[2:]\\n        if start_positions is not None and end_positions is not None:\\n            # If we are on multi-GPU, split add a dimension\\n            if len(start_positions.size()) > 1:\\n                start_positions = start_positions.squeeze(-1)\\n            if len(end_positions.size()) > 1:\\n                end_positions = end_positions.squeeze(-1)\\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\\n            ignored_index = start_logits.size(1)\\n            start_positions.clamp_(0, ignored_index)\\n            end_positions.clamp_(0, ignored_index)\\n\\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\\n            start_loss = loss_fct(start_logits, start_positions)\\n            end_loss = loss_fct(end_logits, end_positions)\\n            total_loss = (start_loss + end_loss) / 2\\n            outputs = (total_loss,) + outputs\\n\\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\\n'"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import module\n",
    "import inspect\n",
    "src = inspect.getsource(BertForQuestionAnswering)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/narine/anaconda3/envs/captum/lib/python3.7/site-packages/transformers/modeling_bert.py'"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getfile(BertForQuestionAnswering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is iterating over all layers it can take over 10 seconds. Please, be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n",
      "torch.Size([1, 26, 768]) torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "layer_attrs_start = []\n",
    "layer_attrs_end = []\n",
    "\n",
    "layer_attrs_start_expanded = {1: [], 23: []}\n",
    "layer_attrs_end_expanded = {1: [], 23: []}\n",
    "#token_to_explain = [13, 3]\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    lc = LayerConductance(squad_pos_forward_func, model.bert.encoder.layer[i])\n",
    "    layer_attributions_start = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(token_type_ids, position_ids,attention_mask, 0))\n",
    "    layer_attributions_end = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(token_type_ids, position_ids,attention_mask, 1))\n",
    "    layer_attrs_start.append(summarize_attributions(layer_attributions_start).cpu().detach().tolist())\n",
    "    layer_attrs_end.append(summarize_attributions(layer_attributions_end).cpu().detach().tolist())\n",
    "\n",
    "    for key in layer_attrs_end_expanded:\n",
    "        layer_attrs_end_expanded[key].append(layer_attributions_end[0,key,:].cpu().detach().tolist())\n",
    "    print(layer_attributions_end.shape, layer_attributions_end[0,key,:].shape)\n",
    "    \n",
    "layer_attrs_start = np.array(layer_attrs_start)\n",
    "layer_attrs_end = np.array(layer_attrs_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(layer_attrs_start_expanded[23]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.boxplot(layer_attrs_end_expanded.transpose()) #, positions=x, notch=True)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax = sns.boxplot(data=layer_attrs_end_expanded[23])\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "xticklabels=text\n",
    "yticklabels=list(range(1,13))\n",
    "ax = sns.heatmap(layer_attrs_start, xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2)\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "xticklabels=text\n",
    "yticklabels=list(range(1,13))\n",
    "ax = sns.heatmap(layer_attrs_end, xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2) #, annot=True\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(model, interpretable_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(numbers, n=5):\n",
    "    maximum=max(numbers)\n",
    "    minimum=min(numbers)\n",
    "    total=len(numbers)\n",
    "\n",
    "    range_=maximum - minimum\n",
    "\n",
    "    step=range_/n\n",
    "    bound=minimum\n",
    "    countdict_pdf= {} # dictionary {bound : number of items between bound and bound+step}\n",
    "    for i in range(0,n):\n",
    "        countdict_pdf[bound]=sum(1 if (bound<x and x<=bound+step) else 0 for x in numbers) / total\n",
    "        bound=bound+step\n",
    "    ret = list(countdict_pdf.values())\n",
    "    if len(ret) < n:\n",
    "        ret.extend([0.0] * (n - len(ret)))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0026041666666666665,\n",
       "  0.028645833333333332,\n",
       "  0.7213541666666666,\n",
       "  0.234375,\n",
       "  0.01171875],\n",
       " [0.0078125,\n",
       "  0.041666666666666664,\n",
       "  0.7630208333333334,\n",
       "  0.1640625,\n",
       "  0.022135416666666668],\n",
       " [0.010416666666666666,\n",
       "  0.5716145833333334,\n",
       "  0.3997395833333333,\n",
       "  0.014322916666666666,\n",
       "  0.0026041666666666665],\n",
       " [0.01171875,\n",
       "  0.11848958333333333,\n",
       "  0.7408854166666666,\n",
       "  0.11848958333333333,\n",
       "  0.009114583333333334],\n",
       " [0.0026041666666666665,\n",
       "  0.061197916666666664,\n",
       "  0.7278645833333334,\n",
       "  0.18489583333333334,\n",
       "  0.022135416666666668],\n",
       " [0.005208333333333333,\n",
       "  0.28125,\n",
       "  0.6432291666666666,\n",
       "  0.05859375,\n",
       "  0.009114583333333334],\n",
       " [0.040364583333333336,\n",
       "  0.6809895833333334,\n",
       "  0.2604166666666667,\n",
       "  0.013020833333333334,\n",
       "  0.00390625],\n",
       " [0.01953125, 0.21875, 0.6653645833333334, 0.08723958333333333, 0.0078125],\n",
       " [0.016927083333333332,\n",
       "  0.7005208333333334,\n",
       "  0.2669270833333333,\n",
       "  0.013020833333333334,\n",
       "  0.0013020833333333333],\n",
       " [0.044270833333333336,\n",
       "  0.62109375,\n",
       "  0.2916666666666667,\n",
       "  0.0390625,\n",
       "  0.0026041666666666665],\n",
       " [0.05078125,\n",
       "  0.6953125,\n",
       "  0.22916666666666666,\n",
       "  0.016927083333333332,\n",
       "  0.006510416666666667],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atts1 = [pdf(layer_attr_end_expanded) for layer_attr_end_expanded in layer_attrs_end_expanded[1]]\n",
    "atts2 = [pdf(layer_attr_end_expanded) for layer_attr_end_expanded in layer_attrs_end_expanded[23]]\n",
    "\n",
    "attr1 = [sum(layer_attr_end_expanded) for layer_attr_end_expanded in layer_attrs_end_expanded[1]]\n",
    "attr2 = [sum(layer_attr_end_expanded) for layer_attr_end_expanded in layer_attrs_end_expanded[23]]\n",
    "\n",
    "atts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1719e-02, 2.0833e-01, 7.2526e-01, 5.2083e-02, 1.3021e-03],\n",
      "        [5.2083e-03, 4.0755e-01, 5.6250e-01, 2.2135e-02, 1.3021e-03],\n",
      "        [3.2552e-02, 6.1589e-01, 3.2682e-01, 2.2135e-02, 1.3021e-03],\n",
      "        [1.4323e-01, 8.4635e-01, 7.8125e-03, 1.0000e-16, 1.3021e-03],\n",
      "        [1.1719e-02, 1.5885e-01, 7.6953e-01, 5.4688e-02, 3.9062e-03],\n",
      "        [1.1589e-01, 8.5807e-01, 2.2135e-02, 1.3021e-03, 1.3021e-03],\n",
      "        [3.9062e-02, 6.3411e-01, 2.9818e-01, 2.4740e-02, 2.6042e-03],\n",
      "        [9.1146e-03, 1.4453e-01, 7.7344e-01, 6.6406e-02, 5.2083e-03],\n",
      "        [5.2083e-03, 9.5052e-02, 7.7865e-01, 1.0938e-01, 1.0417e-02],\n",
      "        [1.0000e-16, 5.2083e-03, 4.2969e-01, 5.4297e-01, 2.0833e-02],\n",
      "        [1.6927e-02, 4.6484e-01, 4.6745e-01, 4.4271e-02, 5.2083e-03],\n",
      "        [1.8750e-01, 7.3698e-01, 6.5104e-02, 6.5104e-03, 2.6042e-03]])\n",
      "tensor([[1.1719e-02, 2.0833e-01, 7.2526e-01, 5.2083e-02, 1.3021e-03],\n",
      "        [5.2083e-03, 4.0755e-01, 5.6250e-01, 2.2135e-02, 1.3021e-03],\n",
      "        [3.2552e-02, 6.1589e-01, 3.2682e-01, 2.2135e-02, 1.3021e-03],\n",
      "        [1.4323e-01, 8.4635e-01, 7.8125e-03, 1.0000e-16, 1.3021e-03],\n",
      "        [1.1719e-02, 1.5885e-01, 7.6953e-01, 5.4688e-02, 3.9062e-03],\n",
      "        [1.1589e-01, 8.5807e-01, 2.2135e-02, 1.3021e-03, 1.3021e-03],\n",
      "        [3.9062e-02, 6.3411e-01, 2.9818e-01, 2.4740e-02, 2.6042e-03],\n",
      "        [9.1146e-03, 1.4453e-01, 7.7344e-01, 6.6406e-02, 5.2083e-03],\n",
      "        [5.2083e-03, 9.5052e-02, 7.7865e-01, 1.0938e-01, 1.0417e-02],\n",
      "        [1.0000e-16, 5.2083e-03, 4.2969e-01, 5.4297e-01, 2.0833e-02],\n",
      "        [1.6927e-02, 4.6484e-01, 4.6745e-01, 4.4271e-02, 5.2083e-03],\n",
      "        [1.8750e-01, 7.3698e-01, 6.5104e-02, 6.5104e-03, 2.6042e-03]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7744, 0.8098, 0.8685, 0.4661, 0.7265, 0.4827, 0.8834, 0.7286, 0.7355,\n",
       "        0.8026, 0.9460, 0.7649])"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#atts1 = torch.exp(-torch.tensor(atts1))\n",
    "#\n",
    "#print(torch.tensor(atts2))\n",
    "atts1 = torch.tensor(atts1)\n",
    "atts1 = 0.9999 * atts1 + (0.0001 / atts1.shape[-1])\n",
    "entropies1= -(atts1 * torch.log(atts1)).sum(-1)#.mean(-1)\n",
    "\n",
    "#attns = 0.9999 * atts + (0.0001 / atts.shape[-1])\n",
    "atts2 = torch.tensor(atts2) + 0.1e-15\n",
    "print(atts2)\n",
    "#atts2 = 0.9999 * atts2 + (0.0001 / atts2.shape[-1])\n",
    "print(atts2)\n",
    "entropies2= -(atts2 * torch.log(atts2)).sum(-1)#.mean(-1)\n",
    "entropies2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7744, 0.8098, 0.8685, 0.4661, 0.7265, 0.4827, 0.8834, 0.7286, 0.7355,\n",
      "        0.8026, 0.9460, 0.7649])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.boxplot(layer_attrs_end_expanded.transpose()) #, positions=x, notch=True)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "#data = {'Attribution': atts1,\n",
    "#        'c': np.arange(12),\n",
    "#        'd': attr1}\n",
    "norm = (entropies2 -  min(entropies2)) / (max(entropies2) - min(entropies2)) *1000\n",
    "plt.scatter(np.arange(12), attr2, s=norm.tolist())\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Attribution')\n",
    "#plt.show()\n",
    "print(entropies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = (30 * np.random.rand(12))**2  # 0 to 15 point radii\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
